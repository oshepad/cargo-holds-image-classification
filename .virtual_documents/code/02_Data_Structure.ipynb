





import os
import shutil

import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator


images_df = pd.read_csv('../data/data_eda.csv')
images_df.head()


# number of images to hold back for test set
test_size = round((images_df[images_df['class'] == 'dirty'].value_counts().sum() / 10), 0)
test_size


# Holding back 10 percent of images from each class to generate a test set.

clean_path = '../data/classified/clean/'
clean_images = [i for i in os.listdir(clean_path)]

dirty_path = '../data/classified/dirty/'
dirty_images = [i for i in os.listdir(dirty_path)]

# get ten random clean images
random.shuffle(clean_images)
test_clean_list = clean_images[:55]

# make a filepath
test_clean = []
for image in test_clean_list:
    test_clean.append(clean_path+image)

# move to test directory
for clean_image in test_clean:
    shutil.move(clean_image, '../data/test/clean/')

# get ten random dirty images
random.shuffle(dirty_images)
test_dirty_list = dirty_images[:55]

# make a filepath
test_dirty = []
for image in test_dirty_list:
    test_dirty.append(dirty_path+image)

# move to test directory
for dirty_image in test_dirty:
    shutil.move(dirty_image, '../data/test/dirty/')


# generating more images from original dataset
balance_datagen = ImageDataGenerator(
    brightness_range=(0.5, 1.5),
    rotation_range=45,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=[0.4, 0.6],
    channel_shift_range=100,
    horizontal_flip=True,
    fill_mode='nearest',
)


def create_synthetics(image_list, target, destination_path):
    """Create more images from a given image list and save to destination"""
    # determine how many images to generate per image
    generate_per_image = target / (len(image_list))

    # loop through image list and generate synthetic images
    for image in image_list:
        try:
            x = load_img(image)
            x = img_to_array(x)
            x = x.reshape((1,) + x.shape)
            i = 0
            for batch in balance_datagen.flow(x, batch_size=1, save_to_dir=destination_path, save_prefix='aug', save_format='jpg'):
                i += 1
                if i > generate_per_image:
                    break  
        except FileNotFoundError:
            print(f'{image} was not found as it was moved to the Test dataset. Skipping ...')
            continue


# Creating more clean images
clean_class_images = [clean_path + file for file in clean_images]
clean_augmented = '../data/augmented/clean/'
clean_target = 375
create_synthetics(clean_class_images, clean_target, clean_augmented)


# Creating more dirty images so model sees some orientation variation
dirty_class_images = [dirty_path + file for file in dirty_images]
dirty_augmented = '../data/augmented/dirty/'
dirty_target = 200
create_synthetics(dirty_class_images, dirty_target, dirty_augmented)


# checking size of augmented classes
num_aug_clean = len([i for i in os.listdir('../data/augmented/clean/')])
num_aug_dirty = len([i for i in os.listdir('../data/augmented/dirty/')])
num_aug_clean, num_aug_dirty


# Pie chart with class distribution
fig, ax = plt.subplots()
ax.pie(x=[num_aug_clean, num_aug_dirty], labels=['Clean', 'Dirty'], colors=['lightblue', 'lightgreen'], autopct='%1.1f%%')
ax.set_title('Class Distribution')
plt.show;


# split the remaining images into training and validation datasets

# training & validation clean
try:
    augmented_clean_path = '../data/augmented/clean/'
    augmented_clean_images = [i for i in os.listdir(augmented_clean_path)]
    random.shuffle(augmented_clean_images)
    
    train_clean = []
    augmented_clean_train = augmented_clean_images[:520]
    for act_image in augmented_clean_train:
        train_clean.append(os.path.join(augmented_clean_path + act_image))
    for train_clean_image in train_clean:
        shutil.move(train_clean_image, '../data/train/clean/')
        
    val_clean = []
    augmented_clean_val = augmented_clean_images[520:]
    for acv_image in augmented_clean_val:
        val_clean.append(os.path.join(augmented_clean_path + acv_image))
    for val_clean_image in val_clean:
        shutil.move(val_clean_image, '../data/validate/clean/')
        
    #training & validation dirty
    augmented_dirty_path = '../data/augmented/dirty/'
    augmented_dirty_images = [i for i in os.listdir(augmented_dirty_path)]
    random.shuffle(augmented_dirty_images)
    
    train_dirty = []
    augmented_dirty_train = augmented_dirty_images[:520]
    for adt_image in augmented_dirty_train:
        train_dirty.append(os.path.join(augmented_dirty_path + adt_image))
    for train_dirty_image in train_dirty:
        shutil.move(train_dirty_image, '../data/train/dirty/')
    
    val_dirty = []
    augmented_dirty_val = augmented_dirty_images[520:]
    for adv_image in augmented_dirty_val:
        val_dirty.append(os.path.join(augmented_dirty_path + adv_image))
    for val_dirty_image in val_dirty:
        shutil.move(val_dirty_image, '../data/validate/dirty/')

except FileNotFoundError as e:
    print(f'Image was not found {e} as it was moved to the Test dataset. Skipping ...')


# check the distribution after the splits
num_train_clean = len([i for i in os.listdir('../data/train/clean/')])
num_train_dirty = len([i for i in os.listdir('../data/train/dirty/')])

num_val_clean = len([i for i in os.listdir('../data/validate/clean/')])
num_val_dirty = len([i for i in os.listdir('../data/validate/dirty/')])

num_test_clean = len([i for i in os.listdir('../data/test/clean/')])
num_test_dirty = len([i for i in os.listdir('../data/test/dirty/')])

num_train_clean, num_train_dirty, num_val_clean, num_val_dirty, num_test_clean, num_test_dirty


# Add new name, not renaming
images_df['new_name'] = images_df['class'] + '_' + images_df.index.astype(str)
images_df.sort_values(by='source', inplace=True)
images_df.head(5)


# Update dataframe to indicate train/validate/test split

# Create lists of where each image was assigned
train_clean_path = '../data/train/clean'
train_clean_images = [i for i in os.listdir(train_clean_path)]
train_dirty_path = '../data/train/dirty'
train_dirty_images = [i for i in os.listdir(train_dirty_path)]
validate_clean_path = '../data/validate/clean'
validate_clean_images = [i for i in os.listdir(validate_clean_path)]
validate_dirty_path = '../data/validate/dirty'
validate_dirty_images = [i for i in os.listdir(validate_dirty_path)]
test_clean_path = '../data/test/clean'
test_clean_images = [i for i in os.listdir(test_clean_path)]
test_dirty_path = '../data/test/dirty'
test_dirty_images = [i for i in os.listdir(test_dirty_path)]


# store them in a dictionary and convert to a dataframe
split_dict = {
    'split': 
    ['train_clean_images'] * len(train_clean_images) + 
    ['train_dirty_images'] * len(train_dirty_images) +
    ['validate_clean_images'] * len(validate_clean_images) + 
    ['validate_dirty_images'] * len(validate_dirty_images) +
    ['test_clean_images'] * len(test_clean_images) + 
    ['test_dirty_images'] * len(test_dirty_images),
    'original_name': 
    train_clean_images + 
    train_dirty_images + 
    validate_clean_images + 
    validate_dirty_images +
    test_clean_images + 
    test_dirty_images
}

split_df = pd.DataFrame(split_dict)
split_df = pd.merge(split_df, images_df, on='original_name', how='left')
split_df['source'] = split_df['source'].fillna('synthetic')
split_df.head()


# Saving data split to a csv
split_df.to_csv('../data/data_split.csv', index=False)
